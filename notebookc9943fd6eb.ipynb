{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":280733444,"sourceType":"kernelVersion"},{"sourceId":289055161,"sourceType":"kernelVersion"},{"sourceId":290482641,"sourceType":"kernelVersion"},{"sourceId":290973334,"sourceType":"kernelVersion"}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# ============================================================\n# INSPECT GATEWAY IMPLEMENTATION\n# ============================================================\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-23T12:50:15.219058Z","iopub.execute_input":"2026-01-23T12:50:15.219926Z","iopub.status.idle":"2026-01-23T12:50:15.624790Z","shell.execute_reply.started":"2026-01-23T12:50:15.219892Z","shell.execute_reply":"2026-01-23T12:50:15.623888Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/aimo-3-utils/wheels.tar.gz\n/kaggle/input/aimo-3-utils/__results__.html\n/kaggle/input/aimo-3-utils/__notebook__.ipynb\n/kaggle/input/aimo-3-utils/__output__.json\n/kaggle/input/aimo-3-utils/custom.css\n/kaggle/input/40-50-gpt-oss-120b-tir-dynamictime-kernelpool/reference.csv\n/kaggle/input/40-50-gpt-oss-120b-tir-dynamictime-kernelpool/submission.parquet\n/kaggle/input/40-50-gpt-oss-120b-tir-dynamictime-kernelpool/__results__.html\n/kaggle/input/40-50-gpt-oss-120b-tir-dynamictime-kernelpool/vllm_server.log\n/kaggle/input/40-50-gpt-oss-120b-tir-dynamictime-kernelpool/__notebook__.ipynb\n/kaggle/input/40-50-gpt-oss-120b-tir-dynamictime-kernelpool/__output__.json\n/kaggle/input/40-50-gpt-oss-120b-tir-dynamictime-kernelpool/custom.css\n/kaggle/input/aimo-3-submission-demo/submission.parquet\n/kaggle/input/aimo-3-submission-demo/__results__.html\n/kaggle/input/aimo-3-submission-demo/__notebook__.ipynb\n/kaggle/input/aimo-3-submission-demo/__output__.json\n/kaggle/input/aimo-3-submission-demo/custom.css\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/AIMO3_Reference_Problems.pdf\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/sample_submission.csv\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/aimo_3_inference_server.py\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/aimo_3_gateway.py\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/__init__.py\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/templates.py\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/relay.py\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/__init__.py\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/ai-mathematical-olympiad-progress-prize-3/kaggle_evaluation/core/generated/__init__.py\n/kaggle/input/aimo3-the-complete-inference-server-starter/submission.parquet\n/kaggle/input/aimo3-the-complete-inference-server-starter/__results__.html\n/kaggle/input/aimo3-the-complete-inference-server-starter/__notebook__.ipynb\n/kaggle/input/aimo3-the-complete-inference-server-starter/__output__.json\n/kaggle/input/aimo3-the-complete-inference-server-starter/custom.css\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"# AIMO3 Dataset Analysis - Complete Overview\\n\",\n        \"\\n\",\n        \"This notebook contains a comprehensive analysis of the AIMO3 (AI Mathematical Olympiad) dataset, including:\\n\",\n        \"- Data loading and exploration\\n\",\n        \"- Answer variable analysis (AIMO3: 0-99999 range) with visualizations\\n\",\n        \"- Feature analysis\\n\",\n        \"- Linear Discriminant Analysis (Classification)\\n\",\n        \"- Regression analysis for answer prediction\\n\",\n        \"- All necessary graphs and visualizations\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"# Install required packages if not already installed\\n\",\n        \"import subprocess\\n\",\n        \"import sys\\n\",\n        \"\\n\",\n        \"def install_package(package):\\n\",\n        \"    try:\\n\",\n        \"        __import__(package)\\n\",\n        \"    except ImportError:\\n\",\n        \"        print(f\\\"Installing {package}...\\\")\\n\",\n        \"        subprocess.check_call([sys.executable, \\\"-m\\\", \\\"pip\\\", \\\"install\\\", package])\\n\",\n        \"        print(f\\\"{package} installed successfully!\\\")\\n\",\n        \"\\n\",\n        \"# Install seaborn if needed\\n\",\n        \"install_package(\\\"seaborn\\\")\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"import numpy as np\\n\",\n        \"import pandas as pd\\n\",\n        \"import matplotlib.pyplot as plt\\n\",\n        \"import seaborn as sns\\n\",\n        \"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\n\",\n        \"from sklearn.preprocessing import StandardScaler\\n\",\n        \"from sklearn.model_selection import train_test_split\\n\",\n        \"from sklearn.linear_model import LinearRegression\\n\",\n        \"from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, classification_report, roc_curve, auc\\n\",\n        \"from scipy import stats\\n\",\n        \"import warnings\\n\",\n        \"warnings.filterwarnings('ignore')\\n\",\n        \"\\n\",\n        \"# Set style for better-looking plots\\n\",\n        \"plt.style.use('seaborn-v0_8-darkgrid')\\n\",\n        \"sns.set_palette(\\\"husl\\\")\\n\",\n        \"\\n\",\n        \"# Initialize train variable to avoid NameError\\n\",\n        \"train = None\\n\",\n        \"\\n\",\n        \"print(\\\"=\\\" * 80)\\n\",\n        \"print(\\\"Dataset Analysis - Data Analysis\\\")\\n\",\n        \"print(\\\"=\\\" * 80)\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 1. Data Loading\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"# Update this path to your AIMO3 reference.csv file location\\n\",\n        \"train_path = '../input/ai-mathematical-olympiad-progress-prize-3/reference.csv'\\n\",\n        \"# Alternative: train_path = 'reference.csv'  # if file is in current directory\\n\",\n        \"\\n\",\n        \"try:\\n\",\n        \"    train = pd.read_csv(train_path)\\n\",\n        \"    print(f\\\"Data loaded: {len(train)} rows, {len(train.columns)} columns\\\")\\n\",\n        \"    print(f\\\"Memory usage: {train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\\")\\n\",\n        \"except FileNotFoundError:\\n\",\n        \"    print(f\\\"Warning: Could not find {train_path}\\\")\\n\",\n        \"    print(\\\"Please update the path to your train.csv file\\\")\\n\",\n        \"    train = None\\n\",\n        \"except Exception as e:\\n\",\n        \"    print(f\\\"Error loading data: {e}\\\")\\n\",\n        \"    train = None\\n\",\n        \"except Exception as e:\\n\",\n        \"    print(f\\\"Error loading data: {e}\\\")\\n\",\n        \"    train = None\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 2. Basic Data Exploration\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"if train is not None:\\n\",\n        \"    print(f\\\"Shape: {train.shape}\\\")\\n\",\n        \"    print(f\\\"Columns: {list(train.columns[:10])}... (showing first 10)\\\")\\n\",\n        \"    print(f\\\"\\\\nData types:\\\\n{train.dtypes.value_counts()}\\\")\\n\",\n        \"    print(f\\\"\\\\nMissing values:\\\\n{train.isnull().sum().sum()} total missing values\\\")\\n\",\n        \"    train.head()\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 3. Answer Variable Analysis (AIMO3)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"if train is not None and 'answer' in train.columns:\\n\",\n        \"    answer = train['answer']\\n\",\n        \"    print(f\\\"The minimum value for answer is: {answer.min()}\\\")\\n\",\n        \"    print(f\\\"The maximum value for answer is: {answer.max()}\\\")\\n\",\n        \"    print(f\\\"The mean value for answer is: {answer.mean():.2f}\\\")\\n\",\n        \"    print(f\\\"The median value for answer is: {answer.median():.2f}\\\")\\n\",\n        \"    print(f\\\"Skew of answer is: {answer.skew():.2f}\\\")\\n\",\n        \"    print(f\\\"Kurtosis of answer is: {answer.kurtosis():.2f}\\\")\\n\",\n        \"    print(f\\\"Standard deviation: {answer.std():.2f}\\\")\\n\",\n        \"    \\n\",\n        \"    # Answer range analysis for AIMO3 (0 to 99999)\\n\",\n        \"    print(f\\\"\\\\nAnswer range: 0 to 99999\\\")\\n\",\n        \"    print(f\\\"Answers in AIME range (0-999): {(answer <= 999).sum()} ({((answer <= 999).sum() / len(answer) * 100):.1f}%)\\\")\\n\",\n        \"    print(f\\\"Unique answers: {answer.nunique()}\\\")\\n\",\n        \"    \\n\",\n        \"    # Create target variable (classification) - positive vs zero\\n\",\n        \"    train['target'] = (train['answer'] > 0).astype(int)\\n\",\n        \"    print(f\\\"\\\\nTarget distribution (classification):\\\")\\n\",\n        \"    print(train['target'].value_counts())\\n\",\n        \"    print(f\\\"Positive class percentage: {train['target'].mean()*100:.2f}%\\\")\\n\",\n        \"elif train is not None:\\n\",\n        \"    print(\\\"Note: 'answer' column not found. Available columns:\\\", list(train.columns))\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 4. Answer Variable Visualizations (AIMO3)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"if train is not None and 'answer' in train.columns:\\n\",\n        \"    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\\n\",\n        \"    fig.suptitle('Answer Variable Analysis (AIMO3)', fontsize=16, fontweight='bold')\\n\",\n        \"    \\n\",\n        \"    answer = train['answer']\\n\",\n        \"    \\n\",\n        \"    # Histogram\\n\",\n        \"    axes[0, 0].hist(answer, bins=min(50, answer.nunique()), edgecolor='black', alpha=0.7)\\n\",\n        \"    axes[0, 0].axvline(answer.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {answer.mean():.0f}')\\n\",\n        \"    axes[0, 0].axvline(answer.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {answer.median():.0f}')\\n\",\n        \"    axes[0, 0].set_xlabel('Answer Value')\\n\",\n        \"    axes[0, 0].set_ylabel('Frequency')\\n\",\n        \"    axes[0, 0].set_title('Distribution of Answer Values')\\n\",\n        \"    axes[0, 0].legend()\\n\",\n        \"    axes[0, 0].grid(True, alpha=0.3)\\n\",\n        \"    \\n\",\n        \"    # Box plot\\n\",\n        \"    axes[0, 1].boxplot(answer, vert=True)\\n\",\n        \"    axes[0, 1].set_ylabel('Answer Value')\\n\",\n        \"    axes[0, 1].set_title('Box Plot of Answer Values')\\n\",\n        \"    axes[0, 1].grid(True, alpha=0.3)\\n\",\n        \"    \\n\",\n        \"    # Q-Q plot for normality check\\n\",\n        \"    stats.probplot(answer, dist=\\\"norm\\\", plot=axes[1, 0])\\n\",\n        \"    axes[1, 0].set_title('Q-Q Plot (Normality Check)')\\n\",\n        \"    axes[1, 0].grid(True, alpha=0.3)\\n\",\n        \"    \\n\",\n        \"    # Answer value scatter by problem index\\n\",\n        \"    axes[1, 1].scatter(range(len(answer)), answer.values, alpha=0.6, s=50)\\n\",\n        \"    axes[1, 1].axhline(999, color='orange', linestyle='--', linewidth=2, label='AIME Range (0-999)')\\n\",\n        \"    axes[1, 1].set_xlabel('Problem Index')\\n\",\n        \"    axes[1, 1].set_ylabel('Answer Value')\\n\",\n        \"    axes[1, 1].set_title('Answer Values by Problem')\\n\",\n        \"    axes[1, 1].legend()\\n\",\n        \"    axes[1, 1].grid(True, alpha=0.3)\\n\",\n        \"    \\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"elif train is not None:\\n\",\n        \"    print(\\\"Note: 'answer' column not found. Available columns:\\\", list(train.columns))\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 5. Weight Analysis (Not applicable for AIMO3)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"# Weight Analysis is not applicable for AIMO3 data\\n\",\n        \"# This section was designed for Jane Street data with weight columns\\n\",\n        \"# AIMO3 data focuses on answer prediction (0-99999 range)\\n\",\n        \"print(\\\"Weight Analysis skipped - not applicable for AIMO3 dataset\\\")\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 6. Daily Ratio Analysis (Not applicable for AIMO3)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"# Daily Ratio Analysis is not applicable for AIMO3 data\\n\",\n        \"# This section was designed for Jane Street data with date, weight, and resp columns\\n\",\n        \"# AIMO3 data uses 'answer' column (0-99999 range) instead\\n\",\n        \"print(\\\"Daily Ratio Analysis skipped - not applicable for AIMO3 dataset\\\")\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 7. Feature Analysis\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"if train is not None:\\n\",\n        \"    # Identify feature columns\\n\",\n        \"    feature_cols = [col for col in train.columns if col.startswith('feature_')]\\n\",\n        \"    print(f\\\"Found {len(feature_cols)} feature columns\\\")\\n\",\n        \"    \\n\",\n        \"    if len(feature_cols) > 0:\\n\",\n        \"        # Identify continuous features\\n\",\n        \"        continuous_features = []\\n\",\n        \"        for col in feature_cols:\\n\",\n        \"            if train[col].dtype in ['float64', 'int64']:\\n\",\n        \"                unique_ratio = train[col].nunique() / len(train)\\n\",\n        \"                if unique_ratio > 0.1:  # More than 10% unique values\\n\",\n        \"                    continuous_features.append(col)\\n\",\n        \"        \\n\",\n        \"        print(f\\\"Found {len(continuous_features)} continuous features\\\")\\n\",\n        \"        print(\\\"Showing only top 10 of continuous features\\\")\\n\",\n        \"        \\n\",\n        \"        # Feature correlation with target\\n\",\n        \"        if 'target' in train.columns:\\n\",\n        \"            correlations = train[continuous_features + ['target']].corr()['target'].abs().sort_values(ascending=False)\\n\",\n        \"            top_features = correlations.head(11).index[1:]  # Exclude 'target' itself\\n\",\n        \"            \\n\",\n        \"            print(f\\\"\\\\nTop 10 features correlated with target:\\\")\\n\",\n        \"            for feat in top_features[:10]:\\n\",\n        \"                corr = correlations[feat]\\n\",\n        \"                print(f\\\"  {feat}: {corr:.4f}\\\")\\n\",\n        \"            \\n\",\n        \"            # Feature importance visualization\\n\",\n        \"            fig, axes = plt.subplots(2, 2, figsize=(18, 12))\\n\",\n        \"            fig.suptitle('Feature Analysis', fontsize=16, fontweight='bold')\\n\",\n        \"            \\n\",\n        \"            # Top correlated features\\n\",\n        \"            top_corr = correlations.head(11)[1:11]  # Top 10 excluding target\\n\",\n        \"            axes[0, 0].barh(range(len(top_corr)), top_corr.values)\\n\",\n        \"            axes[0, 0].set_yticks(range(len(top_corr)))\\n\",\n        \"            axes[0, 0].set_yticklabels([f.replace('feature_', 'f_') for f in top_corr.index])\\n\",\n        \"            axes[0, 0].set_xlabel('Absolute Correlation with Target')\\n\",\n        \"            axes[0, 0].set_title('Top 10 Features Correlated with Target')\\n\",\n        \"            axes[0, 0].grid(True, alpha=0.3, axis='x')\\n\",\n        \"            \\n\",\n        \"            # Feature distribution (sample of top features)\\n\",\n        \"            for i, feat in enumerate(top_corr.head(4).index):\\n\",\n        \"                row = i // 2\\n\",\n        \"                col = (i % 2) + 1\\n\",\n        \"                if row < 2 and col < 2:\\n\",\n        \"                    axes[row, col].hist(train[feat].dropna(), bins=50, edgecolor='black', alpha=0.7)\\n\",\n        \"                    axes[row, col].set_xlabel(feat.replace('feature_', 'f_'))\\n\",\n        \"                    axes[row, col].set_ylabel('Frequency')\\n\",\n        \"                    axes[row, col].set_title(f'Distribution of {feat.replace(\\\"feature_\\\", \\\"f_\\\")}')\\n\",\n        \"                    axes[row, col].grid(True, alpha=0.3)\\n\",\n        \"            \\n\",\n        \"            plt.tight_layout()\\n\",\n        \"            plt.show()\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 8. Linear Discriminant Analysis - Classification\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"if train is not None and 'target' in train.columns and len(continuous_features) > 0:\\n\",\n        \"    print(\\\"Linear Discriminant Analysis for AIMO3 Classification (answer > 0 vs answer = 0)\\\")\\n\",\n        \"    \\n\",\n        \"    # Prepare data\\n\",\n        \"    X = train[continuous_features].fillna(0)\\n\",\n        \"    y = train['target']\\n\",\n        \"    \\n\",\n        \"    # Split data\\n\",\n        \"    X_train, X_test, y_train, y_test = train_test_split(\\n\",\n        \"        X, y, test_size=0.2, random_state=42, stratify=y\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    # Scale features\\n\",\n        \"    scaler = StandardScaler()\\n\",\n        \"    X_train_scaled = scaler.fit_transform(X_train)\\n\",\n        \"    X_test_scaled = scaler.transform(X_test)\\n\",\n        \"    \\n\",\n        \"    # Train LDA\\n\",\n        \"    lda = LinearDiscriminantAnalysis()\\n\",\n        \"    lda.fit(X_train_scaled, y_train)\\n\",\n        \"    \\n\",\n        \"    # Evaluate\\n\",\n        \"    train_score = lda.score(X_train_scaled, y_train)\\n\",\n        \"    test_score = lda.score(X_test_scaled, y_test)\\n\",\n        \"    \\n\",\n        \"    print(f\\\"Linear Discriminant Analysis training set score: {train_score:.3f}\\\")\\n\",\n        \"    print(f\\\"Linear Discriminant Analysis test set score: {test_score:.3f}\\\")\\n\",\n        \"    \\n\",\n        \"    # Predictions\\n\",\n        \"    y_pred = lda.predict(X_test_scaled)\\n\",\n        \"    y_pred_proba = lda.predict_proba(X_test_scaled)[:, 1]\\n\",\n        \"    \\n\",\n        \"    # Confusion matrix visualization\\n\",\n        \"    cm = confusion_matrix(y_test, y_pred)\\n\",\n        \"    \\n\",\n        \"    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\\n\",\n        \"    fig.suptitle('Linear Discriminant Analysis - AIMO3 Classification Results', fontsize=16, fontweight='bold')\\n\",\n        \"    \\n\",\n        \"    # Confusion matrix\\n\",\n        \"    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\\n\",\n        \"    axes[0].set_xlabel('Predicted')\\n\",\n        \"    axes[0].set_ylabel('Actual')\\n\",\n        \"    axes[0].set_title('Confusion Matrix')\\n\",\n        \"    \\n\",\n        \"    # ROC curve\\n\",\n        \"    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\\n\",\n        \"    roc_auc = auc(fpr, tpr)\\n\",\n        \"    \\n\",\n        \"    axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\\n\",\n        \"    axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\\n\",\n        \"    axes[1].set_xlim([0.0, 1.0])\\n\",\n        \"    axes[1].set_ylim([0.0, 1.05])\\n\",\n        \"    axes[1].set_xlabel('False Positive Rate')\\n\",\n        \"    axes[1].set_ylabel('True Positive Rate')\\n\",\n        \"    axes[1].set_title('ROC Curve')\\n\",\n        \"    axes[1].legend(loc=\\\"lower right\\\")\\n\",\n        \"    axes[1].grid(True, alpha=0.3)\\n\",\n        \"    \\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"    \\n\",\n        \"    print(\\\"\\\\nClassification Report:\\\")\\n\",\n        \"    print(classification_report(y_test, y_pred))\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 9. Regression Analysis (AIMO3 Answer Prediction)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"# ============================================================================\\n\",\n        \"# PHENOMENAL AIMO3 REGRESSION ANALYSIS - ALL-IN-ONE CELL\\n\",\n        \"# ============================================================================\\n\",\n        \"# Comprehensive regression analysis with beautiful visualizations\\n\",\n        \"# Based on AIMO3 answer prediction (0-99999 range)\\n\",\n        \"\\n\",\n        \"import numpy as np\\n\",\n        \"import pandas as pd\\n\",\n        \"import matplotlib.pyplot as plt\\n\",\n        \"import seaborn as sns\\n\",\n        \"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\\n\",\n        \"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\\n\",\n        \"from sklearn.preprocessing import StandardScaler\\n\",\n        \"from sklearn.model_selection import train_test_split\\n\",\n        \"from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\\n\",\n        \"import warnings\\n\",\n        \"warnings.filterwarnings('ignore')\\n\",\n        \"\\n\",\n        \"# Set beautiful style\\n\",\n        \"plt.style.use('seaborn-v0_8-darkgrid')\\n\",\n        \"sns.set_palette(\\\"husl\\\")\\n\",\n        \"plt.rcParams['figure.facecolor'] = 'white'\\n\",\n        \"plt.rcParams['axes.facecolor'] = 'white'\\n\",\n        \"\\n\",\n        \"# Check if data is loaded\\n\",\n        \"if train is not None and 'answer' in train.columns:\\n\",\n        \"    # Prepare data\\n\",\n        \"    feature_cols = [col for col in train.columns if col.startswith('feature_')]\\n\",\n        \"    if len(feature_cols) > 0:\\n\",\n        \"        # Identify continuous features\\n\",\n        \"        continuous_features = []\\n\",\n        \"        for col in feature_cols:\\n\",\n        \"            if train[col].dtype in ['float64', 'int64']:\\n\",\n        \"                unique_ratio = train[col].nunique() / len(train)\\n\",\n        \"                if unique_ratio > 0.1:\\n\",\n        \"                    continuous_features.append(col)\\n\",\n        \"        \\n\",\n        \"        if len(continuous_features) > 0:\\n\",\n        \"            X = train[continuous_features].fillna(0)\\n\",\n        \"            y = train['answer']\\n\",\n        \"            \\n\",\n        \"            # Split data\\n\",\n        \"            X_train, X_test, y_train, y_test = train_test_split(\\n\",\n        \"                X, y, test_size=0.2, random_state=42\\n\",\n        \"            )\\n\",\n        \"            \\n\",\n        \"            # Scale features\\n\",\n        \"            scaler = StandardScaler()\\n\",\n        \"            X_train_scaled = scaler.fit_transform(X_train)\\n\",\n        \"            X_test_scaled = scaler.transform(X_test)\\n\",\n        \"            \\n\",\n        \"            # Train multiple models for comparison\\n\",\n        \"            models = {\\n\",\n        \"                'Linear Regression': LinearRegression(),\\n\",\n        \"                'Ridge (L2)': Ridge(alpha=1.0),\\n\",\n        \"                'Lasso (L1)': Lasso(alpha=0.1),\\n\",\n        \"                'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5),\\n\",\n        \"                'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),\\n\",\n        \"                'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5)\\n\",\n        \"            }\\n\",\n        \"            \\n\",\n        \"            results = {}\\n\",\n        \"            predictions = {}\\n\",\n        \"            \\n\",\n        \"            print(\\\"=\\\" * 80)\\n\",\n        \"            print(\\\"PHENOMENAL AIMO3 REGRESSION ANALYSIS\\\")\\n\",\n        \"            print(\\\"=\\\" * 80)\\n\",\n        \"            print(f\\\"\\\\nTraining {len(models)} models on {len(continuous_features)} features...\\\")\\n\",\n        \"            print(f\\\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\\\")\\n\",\n        \"            print(f\\\"Answer range: {y.min()} to {y.max()}\\\")\\n\",\n        \"            print(\\\"\\\\n\\\" + \\\"-\\\" * 80)\\n\",\n        \"            \\n\",\n        \"            # Train all models\\n\",\n        \"            for name, model in models.items():\\n\",\n        \"                model.fit(X_train_scaled, y_train)\\n\",\n        \"                y_pred = model.predict(X_test_scaled)\\n\",\n        \"                predictions[name] = y_pred\\n\",\n        \"                \\n\",\n        \"                mse = mean_squared_error(y_test, y_pred)\\n\",\n        \"                rmse = np.sqrt(mse)\\n\",\n        \"                mae = mean_absolute_error(y_test, y_pred)\\n\",\n        \"                r2 = r2_score(y_test, y_pred)\\n\",\n        \"                \\n\",\n        \"                results[name] = {\\n\",\n        \"                    'MSE': mse,\\n\",\n        \"                    'RMSE': rmse,\\n\",\n        \"                    'MAE': mae,\\n\",\n        \"                    'RÂ²': r2\\n\",\n        \"                }\\n\",\n        \"                \\n\",\n        \"                print(f\\\"{name:25s} | RÂ²: {r2:7.4f} | RMSE: {rmse:10.2f} | MAE: {mae:10.2f}\\\")\\n\",\n        \"            \\n\",\n        \"            print(\\\"-\\\" * 80)\\n\",\n        \"            \\n\",\n        \"            # Select best model\\n\",\n        \"            best_model_name = max(results, key=lambda x: results[x]['RÂ²'])\\n\",\n        \"            best_pred = predictions[best_model_name]\\n\",\n        \"            best_r2 = results[best_model_name]['RÂ²']\\n\",\n        \"            best_rmse = results[best_model_name]['RMSE']\\n\",\n        \"            \\n\",\n        \"            print(f\\\"\\\\nðŸ† BEST MODEL: {best_model_name}\\\")\\n\",\n        \"            print(f\\\"   RÂ² Score: {best_r2:.4f}\\\")\\n\",\n        \"            print(f\\\"   RMSE: {best_rmse:.2f}\\\")\\n\",\n        \"            print(\\\"=\\\" * 80)\\n\",\n        \"            \\n\",\n        \"            # ========================================================================\\n\",\n        \"            # PHENOMENAL VISUALIZATIONS\\n\",\n        \"            # ========================================================================\\n\",\n        \"            \\n\",\n        \"            # Create a comprehensive figure with multiple subplots\\n\",\n        \"            fig = plt.figure(figsize=(20, 16))\\n\",\n        \"            gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\\n\",\n        \"            \\n\",\n        \"            # 1. PREDICTED VS ACTUAL (Top Left - Large)\\n\",\n        \"            ax1 = fig.add_subplot(gs[0:2, 0:2])\\n\",\n        \"            ax1.scatter(y_test, best_pred, alpha=0.6, s=30, c=y_test, cmap='viridis', edgecolors='black', linewidth=0.5)\\n\",\n        \"            ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \\n\",\n        \"                    'r--', lw=3, label='Perfect Prediction', zorder=5)\\n\",\n        \"            ax1.set_xlabel('Actual Answer', fontsize=14, fontweight='bold')\\n\",\n        \"            ax1.set_ylabel('Predicted Answer', fontsize=14, fontweight='bold')\\n\",\n        \"            ax1.set_title(f'Predicted vs Actual Answer\\\\n{best_model_name} (RÂ² = {best_r2:.4f})', \\n\",\n        \"                         fontsize=16, fontweight='bold', pad=15)\\n\",\n        \"            ax1.legend(fontsize=12, loc='upper left')\\n\",\n        \"            ax1.grid(True, alpha=0.3, linestyle='--')\\n\",\n        \"            ax1.set_facecolor('#f8f9fa')\\n\",\n        \"            \\n\",\n        \"            # Add statistics text box\\n\",\n        \"            stats_text = f'RMSE: {best_rmse:.2f}\\\\nMAE: {results[best_model_name][\\\"MAE\\\"]:.2f}'\\n\",\n        \"            ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes, \\n\",\n        \"                    fontsize=11, verticalalignment='top', \\n\",\n        \"                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\\n\",\n        \"            \\n\",\n        \"            # 2. RESIDUAL PLOT (Top Right)\\n\",\n        \"            ax2 = fig.add_subplot(gs[0:2, 2])\\n\",\n        \"            residuals = y_test - best_pred\\n\",\n        \"            ax2.scatter(best_pred, residuals, alpha=0.6, s=30, c=residuals, \\n\",\n        \"                       cmap='RdBu_r', edgecolors='black', linewidth=0.5)\\n\",\n        \"            ax2.axhline(y=0, color='r', linestyle='--', linewidth=3, label='Zero Residual')\\n\",\n        \"            ax2.axhline(y=residuals.std() * 2, color='orange', linestyle=':', linewidth=2, alpha=0.7, label='Â±2Ïƒ')\\n\",\n        \"            ax2.axhline(y=-residuals.std() * 2, color='orange', linestyle=':', linewidth=2, alpha=0.7)\\n\",\n        \"            ax2.set_xlabel('Predicted Answer', fontsize=12, fontweight='bold')\\n\",\n        \"            ax2.set_ylabel('Residuals', fontsize=12, fontweight='bold')\\n\",\n        \"            ax2.set_title('Residual Plot', fontsize=14, fontweight='bold', pad=10)\\n\",\n        \"            ax2.legend(fontsize=10)\\n\",\n        \"            ax2.grid(True, alpha=0.3, linestyle='--')\\n\",\n        \"            ax2.set_facecolor('#f8f9fa')\\n\",\n        \"            \\n\",\n        \"            # 3. RESIDUAL DISTRIBUTION (Middle Left)\\n\",\n        \"            ax3 = fig.add_subplot(gs[2, 0])\\n\",\n        \"            ax3.hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\\n\",\n        \"            ax3.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\\n\",\n        \"            ax3.axvline(residuals.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean: {residuals.mean():.2f}')\\n\",\n        \"            ax3.set_xlabel('Residuals', fontsize=11, fontweight='bold')\\n\",\n        \"            ax3.set_ylabel('Frequency', fontsize=11, fontweight='bold')\\n\",\n        \"            ax3.set_title('Residual Distribution', fontsize=12, fontweight='bold')\\n\",\n        \"            ax3.legend(fontsize=9)\\n\",\n        \"            ax3.grid(True, alpha=0.3, axis='y')\\n\",\n        \"            \\n\",\n        \"            # 4. Q-Q PLOT FOR RESIDUALS (Middle Center)\\n\",\n        \"            from scipy import stats\\n\",\n        \"            ax4 = fig.add_subplot(gs[2, 1])\\n\",\n        \"            stats.probplot(residuals, dist=\\\"norm\\\", plot=ax4)\\n\",\n        \"            ax4.set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')\\n\",\n        \"            ax4.grid(True, alpha=0.3)\\n\",\n        \"            \\n\",\n        \"            # 5. MODEL COMPARISON (Middle Right)\\n\",\n        \"            ax5 = fig.add_subplot(gs[2, 2])\\n\",\n        \"            model_names = list(results.keys())\\n\",\n        \"            r2_scores = [results[m]['RÂ²'] for m in model_names]\\n\",\n        \"            colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(model_names)))\\n\",\n        \"            bars = ax5.barh(model_names, r2_scores, color=colors, edgecolor='black', linewidth=1.5)\\n\",\n        \"            ax5.set_xlabel('RÂ² Score', fontsize=11, fontweight='bold')\\n\",\n        \"            ax5.set_title('Model Comparison (RÂ²)', fontsize=12, fontweight='bold')\\n\",\n        \"            ax5.grid(True, alpha=0.3, axis='x')\\n\",\n        \"            ax5.set_xlim([min(r2_scores) - 0.1, max(r2_scores) + 0.1])\\n\",\n        \"            \\n\",\n        \"            # Highlight best model\\n\",\n        \"            best_idx = model_names.index(best_model_name)\\n\",\n        \"            bars[best_idx].set_edgecolor('gold')\\n\",\n        \"            bars[best_idx].set_linewidth(3)\\n\",\n        \"            \\n\",\n        \"            # 6. ERROR METRICS COMPARISON (Bottom Left)\\n\",\n        \"            ax6 = fig.add_subplot(gs[3, 0])\\n\",\n        \"            metrics = ['RMSE', 'MAE']\\n\",\n        \"            x_pos = np.arange(len(model_names))\\n\",\n        \"            width = 0.35\\n\",\n        \"            \\n\",\n        \"            rmse_vals = [results[m]['RMSE'] for m in model_names]\\n\",\n        \"            mae_vals = [results[m]['MAE'] for m in model_names]\\n\",\n        \"            \\n\",\n        \"            # Normalize for better visualization\\n\",\n        \"            rmse_norm = [v / max(rmse_vals) for v in rmse_vals]\\n\",\n        \"            mae_norm = [v / max(mae_vals) for v in mae_vals]\\n\",\n        \"            \\n\",\n        \"            ax6.bar(x_pos - width/2, rmse_norm, width, label='RMSE (norm)', alpha=0.8, color='coral')\\n\",\n        \"            ax6.bar(x_pos + width/2, mae_norm, width, label='MAE (norm)', alpha=0.8, color='skyblue')\\n\",\n        \"            ax6.set_ylabel('Normalized Error', fontsize=11, fontweight='bold')\\n\",\n        \"            ax6.set_title('Error Metrics Comparison', fontsize=12, fontweight='bold')\\n\",\n        \"            ax6.set_xticks(x_pos)\\n\",\n        \"            ax6.set_xticklabels([m[:10] for m in model_names], rotation=45, ha='right', fontsize=8)\\n\",\n        \"            ax6.legend(fontsize=9)\\n\",\n        \"            ax6.grid(True, alpha=0.3, axis='y')\\n\",\n        \"            \\n\",\n        \"            # 7. PREDICTION ERROR DISTRIBUTION (Bottom Center)\\n\",\n        \"            ax7 = fig.add_subplot(gs[3, 1])\\n\",\n        \"            abs_errors = np.abs(residuals)\\n\",\n        \"            ax7.hist(abs_errors, bins=50, edgecolor='black', alpha=0.7, color='purple')\\n\",\n        \"            ax7.axvline(abs_errors.mean(), color='red', linestyle='--', linewidth=2, \\n\",\n        \"                       label=f'Mean: {abs_errors.mean():.2f}')\\n\",\n        \"            ax7.axvline(abs_errors.median(), color='green', linestyle='--', linewidth=2, \\n\",\n        \"                       label=f'Median: {abs_errors.median():.2f}')\\n\",\n        \"            ax7.set_xlabel('Absolute Error', fontsize=11, fontweight='bold')\\n\",\n        \"            ax7.set_ylabel('Frequency', fontsize=11, fontweight='bold')\\n\",\n        \"            ax7.set_title('Absolute Error Distribution', fontsize=12, fontweight='bold')\\n\",\n        \"            ax7.legend(fontsize=9)\\n\",\n        \"            ax7.grid(True, alpha=0.3, axis='y')\\n\",\n        \"            \\n\",\n        \"            # 8. ACTUAL VS PREDICTED DISTRIBUTION (Bottom Right)\\n\",\n        \"            ax8 = fig.add_subplot(gs[3, 2])\\n\",\n        \"            ax8.hist(y_test, bins=50, alpha=0.5, label='Actual', color='blue', edgecolor='black')\\n\",\n        \"            ax8.hist(best_pred, bins=50, alpha=0.5, label='Predicted', color='orange', edgecolor='black')\\n\",\n        \"            ax8.set_xlabel('Answer Value', fontsize=11, fontweight='bold')\\n\",\n        \"            ax8.set_ylabel('Frequency', fontsize=11, fontweight='bold')\\n\",\n        \"            ax8.set_title('Distribution Comparison', fontsize=12, fontweight='bold')\\n\",\n        \"            ax8.legend(fontsize=10)\\n\",\n        \"            ax8.grid(True, alpha=0.3, axis='y')\\n\",\n        \"            \\n\",\n        \"            # Overall title\\n\",\n        \"            fig.suptitle('PHENOMENAL AIMO3 REGRESSION ANALYSIS - COMPREHENSIVE VISUALIZATION', \\n\",\n        \"                        fontsize=18, fontweight='bold', y=0.995)\\n\",\n        \"            \\n\",\n        \"            plt.tight_layout(rect=[0, 0, 1, 0.99])\\n\",\n        \"            plt.show()\\n\",\n        \"            \\n\",\n        \"            print(\\\"\\\\nâœ… All visualizations generated successfully!\\\")\\n\",\n        \"            print(f\\\"ðŸ“Š Best model ({best_model_name}) achieved RÂ² = {best_r2:.4f}\\\")\\n\",\n        \"            \\n\",\n        \"        else:\\n\",\n        \"            print(\\\"âŒ No continuous features found for regression analysis.\\\")\\n\",\n        \"    else:\\n\",\n        \"        print(\\\"âŒ No feature columns found in the dataset.\\\")\\n\",\n        \"else:\\n\",\n        \"    print(\\\"âŒ Data not loaded or 'answer' column not found.\\\")\\n\",\n        \"    print(\\\"   Please ensure 'train' DataFrame is loaded with 'answer' column.\\\")\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## AIMO3 Inference Server (GPT-OSS-120B)\\n\",\n        \"\\n\",\n        \"**Note:** This section contains inference code for solving AIMO3 problems using GPT-OSS-120B model. This requires specific Kaggle inputs and setup.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"%%time\\n\",\n        \"!find /kaggle/usr/lib -type f -print0 | xargs -0 -P 32 -n 500 cat > /dev/null\\n\",\n        \"\\n\",\n        \"def cache_model(path, exts=(\\\".bin\\\", \\\".pt\\\", \\\".safetensors\\\"), num_workers=None, chunk_mb=256):\\n\",\n        \"    \\\"\\\"\\\"Pre-read model weight files into OS page cache.\\\"\\\"\\\"\\n\",\n        \"    import os\\n\",\n        \"    import multiprocessing\\n\",\n        \"    import time\\n\",\n        \"    from concurrent.futures import ThreadPoolExecutor, as_completed\\n\",\n        \"    \\n\",\n        \"    def warmup_file(fpath):\\n\",\n        \"        chunk_size = chunk_mb * 1024 * 1024\\n\",\n        \"        total = 0\\n\",\n        \"        with open(fpath, \\\"rb\\\") as f:\\n\",\n        \"            while True:\\n\",\n        \"                data = f.read(chunk_size)\\n\",\n        \"                if not data:\\n\",\n        \"                    break\\n\",\n        \"                total += len(data)\\n\",\n        \"        return fpath, total\\n\",\n        \"    \\n\",\n        \"    if os.path.isdir(path):\\n\",\n        \"        files = [\\n\",\n        \"            os.path.join(root, name)\\n\",\n        \"            for root, _, names in os.walk(path)\\n\",\n        \"            for name in names\\n\",\n        \"            if name.endswith(exts)\\n\",\n        \"        ]\\n\",\n        \"        files.sort()\\n\",\n        \"    else:\\n\",\n        \"        files = [path]\\n\",\n        \"    \\n\",\n        \"    if not files:\\n\",\n        \"        raise ValueError(f\\\"No model files found under: {path}\\\")\\n\",\n        \"    \\n\",\n        \"    if num_workers is None:\\n\",\n        \"        try:\\n\",\n        \"            num_workers = min(multiprocessing.cpu_count(), 8)\\n\",\n        \"        except Exception:\\n\",\n        \"            num_workers = 4\\n\",\n        \"    \\n\",\n        \"    print(f\\\"[cache_model] {len(files)} file(s), {num_workers} worker(s)\\\")\\n\",\n        \"    t0 = time.time()\\n\",\n        \"    total_bytes = 0\\n\",\n        \"    \\n\",\n        \"    with ThreadPoolExecutor(max_workers=num_workers) as pool:\\n\",\n        \"        futures = {pool.submit(warmup_file, f): f for f in files}\\n\",\n        \"        for i, fut in enumerate(as_completed(futures), 1):\\n\",\n        \"            fpath, n = fut.result()\\n\",\n        \"            total_bytes += n\\n\",\n        \"            print(f\\\"[{i}/{len(files)}] cached {os.path.basename(fpath)}\\\")\\n\",\n        \"    \\n\",\n        \"    elapsed = time.time() - t0\\n\",\n        \"    gb = total_bytes / 1024**3\\n\",\n        \"    print(f\\\"[cache_model] total read â‰ˆ {gb:.2f} GB in {elapsed:.2f}s\\\")\\n\",\n        \"    return total_bytes\\n\",\n        \"\\n\",\n        \"# Warm up the model weights into cache (faster loading)\\n\",\n        \"cache_model(\\\"/kaggle/input/gpt-oss-120b/transformers/default/1\\\", num_workers=16, chunk_mb=1024)\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"%%time\\n\",\n        \"# Copy vLLM compile cache if available\\n\",\n        \"import os\\n\",\n        \"if os.path.exists(\\\"/kaggle/input/gpt-oss-120b-cache-compile/torch_compile_cache\\\"):\\n\",\n        \"    !mkdir -p /root/.cache/vllm/\\n\",\n        \"    !cp -r /kaggle/input/gpt-oss-120b-cache-compile/torch_compile_cache /root/.cache/vllm/\\n\",\n        \"\\n\",\n        \"uninstall_proc.wait()\\n\",\n        \"subprocess.run([\\\"ls\\\", \\\"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\\\"])\\n\",\n        \"\\n\",\n        \"os.environ[\\\"TRANSFORMERS_NO_TF\\\"] = \\\"1\\\"\\n\",\n        \"os.environ[\\\"TRANSFORMERS_NO_FLAX\\\"] = \\\"1\\\"\\n\",\n        \"os.environ[\\\"TRITON_PTXAS_PATH\\\"] = \\\"/usr/local/cuda/bin/ptxas\\\"\\n\",\n        \"os.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"0\\\"\\n\",\n        \"os.environ[\\\"TOKENIZERS_PARALLELISM\\\"] = \\\"false\\\"\\n\",\n        \"os.environ[\\\"TIKTOKEN_ENCODINGS_BASE\\\"] = \\\"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\\\"\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"%%writefile local_python_tool.py\\n\",\n        \"\\\"\\\"\\\"Python tool using Jupyter kernel for stateful execution.\\\"\\\"\\\"\\n\",\n        \"import os\\n\",\n        \"import queue\\n\",\n        \"import threading\\n\",\n        \"from abc import ABC, abstractmethod\\n\",\n        \"from typing import AsyncIterator, Any\\n\",\n        \"from uuid import UUID, uuid4\\n\",\n        \"from openai_harmony import (\\n\",\n        \"    Author,\\n\",\n        \"    Content,\\n\",\n        \"    Message,\\n\",\n        \"    Role,\\n\",\n        \"    TextContent,\\n\",\n        \"    ToolNamespaceConfig,\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"def add_libs(code: str) -> str:\\n\",\n        \"    \\\"\\\"\\\"Add common math libraries to code.\\\"\\\"\\\"\\n\",\n        \"    return \\\"import math\\\\nimport numpy as np\\\\nimport sympy as sp\\\\nfrom sympy import *\\\\n\\\" + code\\n\",\n        \"\\n\",\n        \"def ensure_last_print(code: str) -> str:\\n\",\n        \"    \\\"\\\"\\\"Ensure the last expression is printed.\\\"\\\"\\\"\\n\",\n        \"    lines = code.strip().split(\\\"\\\\n\\\")\\n\",\n        \"    if lines and \\\"print(\\\" not in lines[-1] and \\\"import\\\" not in lines[-1]:\\n\",\n        \"        if \\\"#\\\" in lines[-1]:\\n\",\n        \"            lines[-1] = lines[-1].split(\\\"#\\\")[0]\\n\",\n        \"        lines[-1] = \\\"print(\\\" + lines[-1] + \\\")\\\"\\n\",\n        \"    return \\\"\\\\n\\\".join(lines)\\n\",\n        \"\\n\",\n        \"class LocalJupyterSession:\\n\",\n        \"    \\\"\\\"\\\"Stateful Jupyter kernel session for code execution.\\\"\\\"\\\"\\n\",\n        \"    # Class-level lock and port counter to avoid port conflicts\\n\",\n        \"    _port_lock = threading.Lock()\\n\",\n        \"    _next_port = 50000\\n\",\n        \"    \\n\",\n        \"    @classmethod\\n\",\n        \"    def _get_next_ports(cls, count: int = 5) -> list[int]:\\n\",\n        \"        \\\"\\\"\\\"Get next available ports for kernel connection.\\\"\\\"\\\"\\n\",\n        \"        with cls._port_lock:\\n\",\n        \"            ports = list(range(cls._next_port, cls._next_port + count))\\n\",\n        \"            cls._next_port += count\\n\",\n        \"        return ports\\n\",\n        \"    \\n\",\n        \"    def __init__(self, connection_file: str | None = None, *, timeout: float = 120.0):\\n\",\n        \"        try:\\n\",\n        \"            from jupyter_client import BlockingKernelClient, KernelManager\\n\",\n        \"        except ImportError as exc:\\n\",\n        \"            raise RuntimeError(\\\"jupyter_client package required\\\") from exc\\n\",\n        \"        \\n\",\n        \"        self._default_timeout = timeout\\n\",\n        \"        self._owns_kernel = False\\n\",\n        \"        self._client: BlockingKernelClient\\n\",\n        \"        self._km: KernelManager | None = None\\n\",\n        \"        \\n\",\n        \"        if connection_file:\\n\",\n        \"            from pathlib import Path\\n\",\n        \"            connection_path = Path(connection_file).expanduser()\\n\",\n        \"            if not connection_path.exists():\\n\",\n        \"                raise FileNotFoundError(f\\\"Connection file not found: {connection_path}\\\")\\n\",\n        \"            client = BlockingKernelClient()\\n\",\n        \"            client.load_connection_file(str(connection_path))\\n\",\n        \"            client.start_channels()\\n\",\n        \"            client.wait_for_ready(timeout=self._default_timeout)\\n\",\n        \"            self._client = client\\n\",\n        \"        else:\\n\",\n        \"            # Allocate unique ports to avoid conflicts when running multiple kernels\\n\",\n        \"            ports = self._get_next_ports(5)\\n\",\n        \"            km = KernelManager()\\n\",\n        \"            km.shell_port = ports[0]\\n\",\n        \"            km.iopub_port = ports[1]\\n\",\n        \"            km.stdin_port = ports[2]\\n\",\n        \"            km.hb_port = ports[3]\\n\",\n        \"            km.control_port = ports[4]\\n\",\n        \"            km.start_kernel()\\n\",\n        \"            client = km.blocking_client()\\n\",\n        \"            client.start_channels()\\n\",\n        \"            client.wait_for_ready(timeout=self._default_timeout)\\n\",\n        \"            self._client = client\\n\",\n        \"            self._km = km\\n\",\n        \"            self._owns_kernel = True\\n\",\n        \"    \\n\",\n        \"    def execute(self, code: str, *, timeout: float | None = None) -> str:\\n\",\n        \"        \\\"\\\"\\\"Execute code and return combined stdout/stderr.\\\"\\\"\\\"\\n\",\n        \"        client = self._client\\n\",\n        \"        effective_timeout = timeout or self._default_timeout\\n\",\n        \"        msg_id = client.execute(code, store_history=True, allow_stdin=False, stop_on_error=False)\\n\",\n        \"        \\n\",\n        \"        stdout_parts: list[str] = []\\n\",\n        \"        stderr_parts: list[str] = []\\n\",\n        \"        \\n\",\n        \"        while True:\\n\",\n        \"            try:\\n\",\n        \"                msg = client.get_iopub_msg(timeout=effective_timeout)\\n\",\n        \"            except queue.Empty as exc:\\n\",\n        \"                raise TimeoutError(\\\"Timed out waiting for kernel output.\\\") from exc\\n\",\n        \"            \\n\",\n        \"            if msg.get(\\\"parent_header\\\", {}).get(\\\"msg_id\\\") != msg_id:\\n\",\n        \"                continue\\n\",\n        \"            \\n\",\n        \"            msg_type = msg.get(\\\"msg_type\\\")\\n\",\n        \"            content = msg.get(\\\"content\\\", {})\\n\",\n        \"            \\n\",\n        \"            if msg_type == \\\"stream\\\":\\n\",\n        \"                text = content.get(\\\"text\\\", \\\"\\\")\\n\",\n        \"                if content.get(\\\"name\\\") == \\\"stdout\\\":\\n\",\n        \"                    stdout_parts.append(text)\\n\",\n        \"                else:\\n\",\n        \"                    stderr_parts.append(text)\\n\",\n        \"            elif msg_type == \\\"error\\\":\\n\",\n        \"                traceback_data = content.get(\\\"traceback\\\")\\n\",\n        \"                if traceback_data:\\n\",\n        \"                    stderr_parts.append(\\\"\\\\n\\\".join(traceback_data))\\n\",\n        \"                else:\\n\",\n        \"                    ename = content.get(\\\"ename\\\", \\\"\\\")\\n\",\n        \"                    evalue = content.get(\\\"evalue\\\", \\\"\\\")\\n\",\n        \"                    stderr_parts.append(f\\\"{ename}: {evalue}\\\".strip())\\n\",\n        \"            elif msg_type in {\\\"execute_result\\\", \\\"display_data\\\"}:\\n\",\n        \"                data = content.get(\\\"data\\\", {})\\n\",\n        \"                text = data.get(\\\"text/plain\\\")\\n\",\n        \"                if text:\\n\",\n        \"                    stdout_parts.append(text if text.endswith(\\\"\\\\n\\\") else f\\\"{text}\\\\n\\\")\\n\",\n        \"            elif msg_type == \\\"status\\\" and content.get(\\\"execution_state\\\") == \\\"idle\\\":\\n\",\n        \"                break\\n\",\n        \"        \\n\",\n        \"        # Drain shell channel\\n\",\n        \"        while True:\\n\",\n        \"            try:\\n\",\n        \"                reply = client.get_shell_msg(timeout=effective_timeout)\\n\",\n        \"            except queue.Empty as exc:\\n\",\n        \"                raise TimeoutError(\\\"Timed out waiting for execution reply.\\\") from exc\\n\",\n        \"            \\n\",\n        \"            if reply.get(\\\"parent_header\\\", {}).get(\\\"msg_id\\\") != msg_id:\\n\",\n        \"                continue\\n\",\n        \"            \\n\",\n        \"            reply_content = reply.get(\\\"content\\\", {})\\n\",\n        \"            if reply_content.get(\\\"status\\\") == \\\"error\\\":\\n\",\n        \"                traceback_data = reply_content.get(\\\"traceback\\\")\\n\",\n        \"                if traceback_data:\\n\",\n        \"                    stderr_parts.append(\\\"\\\\n\\\".join(traceback_data))\\n\",\n        \"                else:\\n\",\n        \"                    ename = reply_content.get(\\\"ename\\\", \\\"\\\")\\n\",\n        \"                    evalue = reply_content.get(\\\"evalue\\\", \\\"\\\")\\n\",\n        \"                    stderr_parts.append(f\\\"{ename}: {evalue}\\\".strip())\\n\",\n        \"            break\\n\",\n        \"        \\n\",\n        \"        stdout = \\\"\\\".join(stdout_parts)\\n\",\n        \"        stderr = \\\"\\\".join(stderr_parts)\\n\",\n        \"        if stderr:\\n\",\n        \"            stdout = f\\\"{stdout.rstrip()}\\\\n{stderr}\\\" if stdout else stderr\\n\",\n        \"        if not stdout.strip():\\n\",\n        \"            stdout = \\\"[WARN] No output. Use print() to see results.\\\"\\n\",\n        \"        return stdout\\n\",\n        \"    \\n\",\n        \"    def close(self):\\n\",\n        \"        import contextlib\\n\",\n        \"        with contextlib.suppress(Exception):\\n\",\n        \"            self._client.stop_channels()\\n\",\n        \"        if self._owns_kernel and self._km is not None:\\n\",\n        \"            with contextlib.suppress(Exception):\\n\",\n        \"                self._km.shutdown_kernel(now=True)\\n\",\n        \"    \\n\",\n        \"    def __del__(self):\\n\",\n        \"        self.close()\\n\",\n        \"\\n\",\n        \"class PythonTool:\\n\",\n        \"    \\\"\\\"\\\"Python execution tool using Jupyter kernel.\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    def __init__(self, execution_backend: str | None = None, local_jupyter_timeout: float = 60.0):\\n\",\n        \"        self._local_jupyter_timeout = local_jupyter_timeout\\n\",\n        \"        self._execution_lock = threading.Lock()\\n\",\n        \"        self._jupyter_session: LocalJupyterSession | None = None\\n\",\n        \"        # Lazy initialization to avoid port conflicts during object creation\\n\",\n        \"        self._init_lock = threading.Lock()\\n\",\n        \"    \\n\",\n        \"    def _ensure_session(self):\\n\",\n        \"        \\\"\\\"\\\"Lazily initialize the Jupyter session.\\\"\\\"\\\"\\n\",\n        \"        if self._jupyter_session is None:\\n\",\n        \"            with self._init_lock:\\n\",\n        \"                if self._jupyter_session is None:\\n\",\n        \"                    self._jupyter_session = LocalJupyterSession(timeout=self._local_jupyter_timeout)\\n\",\n        \"    \\n\",\n        \"    @classmethod\\n\",\n        \"    def get_tool_name(cls) -> str:\\n\",\n        \"        return \\\"python\\\"\\n\",\n        \"    \\n\",\n        \"    @property\\n\",\n        \"    def name(self) -> str:\\n\",\n        \"        return self.get_tool_name()\\n\",\n        \"    \\n\",\n        \"    @property\\n\",\n        \"    def instruction(self) -> str:\\n\",\n        \"        return \\\"\\\"\\\"Use this tool to execute Python code. The code runs in a stateful Jupyter notebook. Use print() to see output.\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    @property\\n\",\n        \"    def tool_config(self) -> ToolNamespaceConfig:\\n\",\n        \"        return ToolNamespaceConfig(\\n\",\n        \"            name=self.get_tool_name(),\\n\",\n        \"            description=self.instruction,\\n\",\n        \"            tools=[]\\n\",\n        \"        )\\n\",\n        \"    \\n\",\n        \"    def _make_response(self, output: str, channel: str | None = None) -> Message:\\n\",\n        \"        content = TextContent(text=output)\\n\",\n        \"        author = Author(role=Role.TOOL, name=self.get_tool_name())\\n\",\n        \"        message = Message(author=author, content=[content]).with_recipient(\\\"assistant\\\")\\n\",\n        \"        if channel:\\n\",\n        \"            message = message.with_channel(channel)\\n\",\n        \"        return message\\n\",\n        \"    \\n\",\n        \"    def process_sync_plus(self, message: Message) -> list[Message]:\\n\",\n        \"        \\\"\\\"\\\"Execute code from message using Jupyter kernel.\\\"\\\"\\\"\\n\",\n        \"        self._ensure_session()\\n\",\n        \"        script = message.content[0].text\\n\",\n        \"        \\n\",\n        \"        with self._execution_lock:\\n\",\n        \"            try:\\n\",\n        \"                output = self._jupyter_session.execute(script)\\n\",\n        \"            except TimeoutError as exc:\\n\",\n        \"                # NEW: timeout -> reset kernel so it won't stay stuck for next calls\\n\",\n        \"                try:\\n\",\n        \"                    self.reset()\\n\",\n        \"                except Exception:\\n\",\n        \"                    pass\\n\",\n        \"                output = f\\\"[ERROR] {exc}\\\"\\n\",\n        \"        \\n\",\n        \"        return [self._make_response(output, channel=message.channel)]\\n\",\n        \"    \\n\",\n        \"    #====================\\n\",\n        \"    def reset(self):\\n\",\n        \"        \\\"\\\"\\\"Hard reset: kill current kernel and start a fresh one.\\\"\\\"\\\"\\n\",\n        \"        with self._execution_lock:\\n\",\n        \"            if self._jupyter_session is not None:\\n\",\n        \"                self._jupyter_session.close()\\n\",\n        \"                self._jupyter_session = None\\n\",\n        \"            self._ensure_session()\\n\",\n        \"    \\n\",\n        \"    def interrupt(self):\\n\",\n        \"        \\\"\\\"\\\"Try a soft interrupt if possible; fallback to hard reset.\\\"\\\"\\\"\\n\",\n        \"        self._ensure_session()\\n\",\n        \"        try:\\n\",\n        \"            km = getattr(self._jupyter_session, \\\"_km\\\", None)\\n\",\n        \"            if km is not None:\\n\",\n        \"                km.interrupt_kernel()\\n\",\n        \"                return\\n\",\n        \"        except Exception:\\n\",\n        \"            pass\\n\",\n        \"        # interrupt\\n\",\n        \"        self.reset()\\n\",\n        \"    #====================\\n\",\n        \"    \\n\",\n        \"    def close(self):\\n\",\n        \"        if self._jupyter_session is not None:\\n\",\n        \"            self._jupyter_session.close()\\n\",\n        \"            self._jupyter_session = None\\n\",\n        \"    \\n\",\n        \"    def __del__(self):\\n\",\n        \"        self.close()\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"import warnings\\n\",\n        \"warnings.simplefilter('ignore')\\n\",\n        \"import re\\n\",\n        \"import math\\n\",\n        \"import threading\\n\",\n        \"import time\\n\",\n        \"import subprocess\\n\",\n        \"from collections import Counter\\n\",\n        \"from concurrent.futures import ThreadPoolExecutor, as_completed\\n\",\n        \"from typing import List\\n\",\n        \"import pandas as pd\\n\",\n        \"import polars as pl\\n\",\n        \"from openai import OpenAI\\n\",\n        \"from transformers import set_seed, AutoTokenizer\\n\",\n        \"from openai_harmony import (\\n\",\n        \"    HarmonyEncodingName,\\n\",\n        \"    load_harmony_encoding,\\n\",\n        \"    Conversation,\\n\",\n        \"    Message,\\n\",\n        \"    Role,\\n\",\n        \"    SystemContent,\\n\",\n        \"    ReasoningEffort,\\n\",\n        \"    RenderConversationConfig,\\n\",\n        \")\\n\",\n        \"from local_python_tool import PythonTool\\n\",\n        \"\\n\",\n        \"# Load Harmony encoding for GPT-OSS\\n\",\n        \"encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\\n\",\n        \"\\n\",\n        \"# Constants\\n\",\n        \"SEED = 42\\n\",\n        \"set_seed(SEED)\\n\",\n        \"MAX_LEN = 64 * 1024\\n\",\n        \"USE_BUDGET = False  # forced to be false\\n\",\n        \"K = 8  # Number of parallel samples\\n\",\n        \"\\n\",\n        \"# Inference parameters\\n\",\n        \"TEMPERATURE = 1.0\\n\",\n        \"TOP_P = 1.0\\n\",\n        \"MIN_P = 0.02\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"def start_vllm_server() -> subprocess.Popen:\\n\",\n        \"    \\\"\\\"\\\"Start vLLM server in background.\\\"\\\"\\\"\\n\",\n        \"    command = [\\n\",\n        \"        \\\"python\\\", \\\"-m\\\", \\\"vllm.entrypoints.openai.api_server\\\",\\n\",\n        \"        \\\"--model\\\", \\\"/kaggle/input/gpt-oss-120b/transformers/default/1\\\",\\n\",\n        \"        \\\"--served-model-name\\\", \\\"gpt-oss\\\",\\n\",\n        \"        \\\"--tensor-parallel-size\\\", \\\"1\\\",\\n\",\n        \"        \\\"--max-num-seqs\\\", \\\"64\\\",\\n\",\n        \"        \\\"--gpu-memory-utilization\\\", \\\"0.96\\\",\\n\",\n        \"        \\\"--host\\\", \\\"0.0.0.0\\\",\\n\",\n        \"        \\\"--port\\\", \\\"8000\\\",\\n\",\n        \"        \\\"--dtype\\\", \\\"auto\\\",\\n\",\n        \"        \\\"--max-model-len\\\", str(MAX_LEN),\\n\",\n        \"        \\\"--stream-interval\\\", \\\"20\\\",\\n\",\n        \"    ]\\n\",\n        \"    with open(\\\"./vllm.log\\\", \\\"w\\\") as logfile:\\n\",\n        \"        process = subprocess.Popen(\\n\",\n        \"            command, stdout=logfile, stderr=subprocess.STDOUT, start_new_session=True\\n\",\n        \"        )\\n\",\n        \"    print(\\\"vLLM server started. Logs: ./vllm.log\\\")\\n\",\n        \"    return process\\n\",\n        \"\\n\",\n        \"vllm_process = start_vllm_server()\\n\",\n        \"\\n\",\n        \"# TIR Prompts\\n\",\n        \"TIR_PROMPT_SIMPLE = \\\"\\\"\\\"Please reason step by step and use the python tool to solve the math problem. Finally, Return only the verified final answer in \\\\\\\\boxed{}, where the answer is an integer in [0, 99999]. Never guess.\\\"\\\"\\\"\\n\",\n        \"\\n\",\n        \"TIR_PROMPT_ENHANCED = \\\"\\\"\\\"Please reason step by step and use the python tool to solve the math problem. For extremely large numbers, find patterns from small cases instead of direct computation. Finally, Return only the verified final answer in \\\\\\\\boxed{}, where the answer is an integer in [0, 99999]. Never guess.\\\"\\\"\\\"\\n\",\n        \"\\n\",\n        \"TIR_PROMPTS = [TIR_PROMPT_SIMPLE]\\n\",\n        \"\\n\",\n        \"# Create Python tool pool\\n\",\n        \"import queue\\n\",\n        \"python_pool = queue.Queue(maxsize=K)\\n\",\n        \"for _ in range(K):\\n\",\n        \"    t = PythonTool(execution_backend=\\\"jupyter\\\", local_jupyter_timeout=60.0)\\n\",\n        \"    t._ensure_session()\\n\",\n        \"    python_pool.put(t)\\n\",\n        \"print(\\\"Pool created!\\\")\\n\",\n        \"\\n\",\n        \"import gc\\n\",\n        \"CLEANUP_CODE = r\\\"\\\"\\\"\\n\",\n        \"import gc\\n\",\n        \"_keep = {\\n\",\n        \"    \\\"__builtins__\\\",\\n\",\n        \"    \\\"__name__\\\",\\n\",\n        \"    \\\"__doc__\\\",\\n\",\n        \"    \\\"__package__\\\",\\n\",\n        \"    \\\"__loader__\\\",\\n\",\n        \"    \\\"__spec__\\\",\\n\",\n        \"    \\\"np\\\",\\n\",\n        \"    \\\"sp\\\",\\n\",\n        \"    \\\"math\\\",\\n\",\n        \"}\\n\",\n        \"g = globals()\\n\",\n        \"for k in list(g.keys()):\\n\",\n        \"    if k in _keep or k.startswith(\\\"_\\\"):\\n\",\n        \"        continue\\n\",\n        \"    try:\\n\",\n        \"        del g[k]\\n\",\n        \"    except Exception:\\n\",\n        \"        pass\\n\",\n        \"gc.collect()\\n\",\n        \"\\\"\\\"\\\"\\n\",\n        \"print(\\\"yes\\\")\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"class HarmonyTIRInferencer:\\n\",\n        \"    \\\"\\\"\\\"Inferencer using Harmony protocol with Tool-Integrated Reasoning (TIR).\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    def __init__(\\n\",\n        \"        self,\\n\",\n        \"        model_path: str,\\n\",\n        \"        max_model_len: int = MAX_LEN,\\n\",\n        \"        temperature: float = TEMPERATURE,\\n\",\n        \"        top_p: float = TOP_P,\\n\",\n        \"        min_p: float = MIN_P,\\n\",\n        \"        seed: int = SEED,\\n\",\n        \"        k: int = K,\\n\",\n        \"        use_budget: bool = USE_BUDGET,\\n\",\n        \"        max_iter: int = 100,\\n\",\n        \"    ):\\n\",\n        \"        self.model_path = model_path\\n\",\n        \"        self.model = \\\"gpt-oss\\\"\\n\",\n        \"        self.max_model_len = max_model_len\\n\",\n        \"        self.temperature = temperature\\n\",\n        \"        self.top_p = top_p\\n\",\n        \"        self.min_p = min_p\\n\",\n        \"        self.seed = seed\\n\",\n        \"        self.k = k\\n\",\n        \"        self.use_budget = use_budget\\n\",\n        \"        self.max_iter = max_iter\\n\",\n        \"        self.base_budget = 60 * 5.5  # 5.5 minutes base per problem\\n\",\n        \"        self.budget = 370  # initial budget in seconds (~6.1 min for first problem)\\n\",\n        \"        self.deadline = None\\n\",\n        \"        \\n\",\n        \"        # Initialize the OpenAI-compatible client pointing to local vLLM server\\n\",\n        \"        self.client = OpenAI(\\n\",\n        \"            base_url=\\\"http://127.0.0.1:8000/v1\\\",\\n\",\n        \"            api_key=\\\"sk-local\\\",\\n\",\n        \"            timeout=360,\\n\",\n        \"        )\\n\",\n        \"        self.stop_token_ids = encoding.stop_tokens_for_assistant_actions()\\n\",\n        \"        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\\n\",\n        \"    \\n\",\n        \"    def wait_server(self):\\n\",\n        \"        \\\"\\\"\\\"Wait until the vLLM server is ready to accept requests.\\\"\\\"\\\"\\n\",\n        \"        for _ in range(15 * 60):\\n\",\n        \"            time.sleep(1)\\n\",\n        \"            try:\\n\",\n        \"                # List models to check if server is up\\n\",\n        \"                print(self.client.models.list())\\n\",\n        \"                return\\n\",\n        \"            except Exception:\\n\",\n        \"                continue\\n\",\n        \"        raise RuntimeError(\\\"vLLM server failed to start\\\")\\n\",\n        \"    \\n\",\n        \"    def get_num_samples(self) -> int:\\n\",\n        \"        \\\"\\\"\\\"Determine number of parallel samples to generate based on remaining budget.\\\"\\\"\\\"\\n\",\n        \"        if not self.use_budget:\\n\",\n        \"            print(f\\\"Budget disabled -> N: {self.k}\\\")\\n\",\n        \"            return self.k\\n\",\n        \"        estimated = (self.budget - 190) / 90\\n\",\n        \"        ret = min(self.k, math.floor(estimated))\\n\",\n        \"        print(f\\\"Budget: {self.budget} -> N: {ret}\\\")\\n\",\n        \"        return max(4, ret)\\n\",\n        \"    \\n\",\n        \"    def apply_chat_template(self, prompt: str, python_tool: PythonTool) -> list[Message]:\\n\",\n        \"        \\\"\\\"\\\"Wrap user prompt into Harmony conversation format with system and tool info.\\\"\\\"\\\"\\n\",\n        \"        return [\\n\",\n        \"            Message.from_role_and_content(\\n\",\n        \"                Role.SYSTEM,\\n\",\n        \"                SystemContent.new()\\n\",\n        \"                .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\\n\",\n        \"                .with_tools(python_tool.tool_config)\\n\",\n        \"            ),\\n\",\n        \"            Message.from_role_and_content(Role.USER, prompt),\\n\",\n        \"        ]\\n\",\n        \"    \\n\",\n        \"    def format_prompts(self, problem: str) -> list[str]:\\n\",\n        \"        \\\"\\\"\\\"Create multiple prompts (possibly with different TIR strategies) for one problem.\\\"\\\"\\\"\\n\",\n        \"        num_samples = self.get_num_samples()\\n\",\n        \"        prompts = []\\n\",\n        \"        for i in range(num_samples):\\n\",\n        \"            # Alternate between the prompt templates for diversity\\n\",\n        \"            tir_prompt = TIR_PROMPTS[i % len(TIR_PROMPTS)]\\n\",\n        \"            prompts.append(problem + \\\"\\\\n\\\\n\\\" + tir_prompt)\\n\",\n        \"        return prompts\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"    def inference(self, problem: str, deadline: float) -> tuple[int, float]:\\n\",\n        \"        \\\"\\\"\\\"Run the multi-sample inference for a single problem and return the final answer and saved time.\\\"\\\"\\\"\\n\",\n        \"        self.deadline = deadline\\n\",\n        \"        start_time = time.time()\\n\",\n        \"        prompts = self.format_prompts(problem)\\n\",\n        \"        responses = self._inference_parallel(prompts)\\n\",\n        \"        duration = time.time() - start_time\\n\",\n        \"        saved_time = max(0.0, deadline - time.time())\\n\",\n        \"        \\n\",\n        \"        print(f\\\"[Budget]: {(deadline - start_time):.2f}s\\\")\\n\",\n        \"        print(f\\\"[inference] Took {duration:.2f}s\\\")\\n\",\n        \"        print(f\\\"[Saved time]: {saved_time:.2f}s\\\")\\n\",\n        \"        \\n\",\n        \"        if self.use_budget:\\n\",\n        \"            budget_left = max(0, self.budget - duration)\\n\",\n        \"            self.budget = self.base_budget + budget_left\\n\",\n        \"            print(f\\\"[inference] Updated budget: {self.budget:.2f}s\\\")\\n\",\n        \"        \\n\",\n        \"        return self.parse_responses(responses), saved_time\\n\",\n        \"    \\n\",\n        \"    def extract_boxed_text(self, text: str) -> int | None:\\n\",\n        \"        \\\"\\\"\\\"Extract a numeric answer from '\\\\\\\\boxed{}' or 'final answer is ...' in the text.\\\"\\\"\\\"\\n\",\n        \"        # Pattern for \\\\boxed{NUMBER}\\n\",\n        \"        pattern = r'oxed{(.*?)}'\\n\",\n        \"        matches = re.findall(pattern, str(text))\\n\",\n        \"        if matches:\\n\",\n        \"            for match in reversed(matches):\\n\",\n        \"                if match:\\n\",\n        \"                    try:\\n\",\n        \"                        # Remove commas/spaces and parse as number\\n\",\n        \"                        clean_match = match.strip().replace(',', '').replace(' ', '')\\n\",\n        \"                        val = int(float(clean_match[:20]))\\n\",\n        \"                        if 0 <= val <= 99999:\\n\",\n        \"                            return val\\n\",\n        \"                    except Exception:\\n\",\n        \"                        pass\\n\",\n        \"        \\n\",\n        \"        # Pattern for \\\"final answer is X\\\" or \\\"Final Answer: X\\\"\\n\",\n        \"        pattern = r'(?i)final\\\\s+answer\\\\s*(?:is|:)?\\\\s*(\\\\d+)'\\n\",\n        \"        matches = re.findall(pattern, text)\\n\",\n        \"        if matches:\\n\",\n        \"            for match in reversed(matches):\\n\",\n        \"                if match:\\n\",\n        \"                    try:\\n\",\n        \"                        val = int(match)\\n\",\n        \"                        if 0 <= val <= 99999:\\n\",\n        \"                            return val\\n\",\n        \"                    except Exception:\\n\",\n        \"                        pass\\n\",\n        \"        \\n\",\n        \"        return None\\n\",\n        \"    \\n\",\n        \"    def parse_responses(self, responses: list[str]) -> int:\\n\",\n        \"        \\\"\\\"\\\"Decide on the final answer from all responses by majority vote (with tie-break).\\\"\\\"\\\"\\n\",\n        \"        answers = [self.extract_boxed_text(r) for r in responses]\\n\",\n        \"        # Filter out any None values\\n\",\n        \"        valid_answers = [a for a in answers if a is not None]\\n\",\n        \"        \\n\",\n        \"        if not valid_answers:\\n\",\n        \"            print(\\\"No valid answers found\\\")\\n\",\n        \"            return 8687\\n\",\n        \"        \\n\",\n        \"        counter = Counter(valid_answers)\\n\",\n        \"        print(f\\\"Answers: {counter}\\\")\\n\",\n        \"        \\n\",\n        \"        # Majority vote: pick the most common answer; break ties by choosing the largest answer\\n\",\n        \"        most_common_list = counter.most_common(2)\\n\",\n        \"        if len(most_common_list) > 1 and most_common_list[0][1] == most_common_list[1][1]:\\n\",\n        \"            tied_answers = [ans for ans, cnt in counter.items() if cnt == most_common_list[0][1]]\\n\",\n        \"            answer = max(tied_answers)\\n\",\n        \"        else:\\n\",\n        \"            answer = most_common_list[0][0]\\n\",\n        \"        \\n\",\n        \"        return answer\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"    def single_generate_tir(self, prompt: str, stop_event: threading.Event, seed_offset: int = 0) -> str:\\n\",\n        \"        \\\"\\\"\\\"Generate a single reasoning trace (with tool use) for the prompt.\\\"\\\"\\\"\\n\",\n        \"        python_tool = None\\n\",\n        \"        borrowed_from_pool = False\\n\",\n        \"        \\n\",\n        \"        def _get_pool():\\n\",\n        \"            return getattr(self, \\\"python_pool\\\", None) or globals().get(\\\"python_pool\\\", None)\\n\",\n        \"        \\n\",\n        \"        def _recreate_tool(close_old: bool = True):\\n\",\n        \"            nonlocal python_tool\\n\",\n        \"            if close_old and python_tool is not None:\\n\",\n        \"                try:\\n\",\n        \"                    python_tool.close()\\n\",\n        \"                except Exception:\\n\",\n        \"                    pass\\n\",\n        \"            python_tool = PythonTool(execution_backend=\\\"jupyter\\\", local_jupyter_timeout=60.0)\\n\",\n        \"            python_tool._ensure_session()\\n\",\n        \"        \\n\",\n        \"        def _time_left():\\n\",\n        \"            if not self.deadline:\\n\",\n        \"                return None\\n\",\n        \"            return self.deadline - time.time()\\n\",\n        \"        \\n\",\n        \"        try:\\n\",\n        \"            # Borrow PythonTool from pool if exists\\n\",\n        \"            pool = _get_pool()\\n\",\n        \"            if isinstance(pool, queue.Queue):\\n\",\n        \"                print(\\\"pool.qsize=\\\", pool.qsize())\\n\",\n        \"                python_tool = pool.get(timeout=5)\\n\",\n        \"                borrowed_from_pool = True\\n\",\n        \"            else:\\n\",\n        \"                python_tool = PythonTool(execution_backend=\\\"jupyter\\\")\\n\",\n        \"            \\n\",\n        \"            # Ensure session exists\\n\",\n        \"            try:\\n\",\n        \"                python_tool._ensure_session()\\n\",\n        \"            except Exception as e:\\n\",\n        \"                print(f\\\"âš ï¸ ensure_session failed: {e} -> recreate kernel now\\\")\\n\",\n        \"                _recreate_tool(close_old=True)\\n\",\n        \"            \\n\",\n        \"            # Cleanup at start\\n\",\n        \"            try:\\n\",\n        \"                python_tool._jupyter_session.execute(CLEANUP_CODE, timeout=5)\\n\",\n        \"            except Exception as e:\\n\",\n        \"                print(f\\\"âš ï¸ Cleanup failed: {e} -> recreate kernel now\\\")\\n\",\n        \"                _recreate_tool(close_old=True)\\n\",\n        \"            \\n\",\n        \"            messages = self.apply_chat_template(prompt, python_tool)\\n\",\n        \"            final_answer_found = \\\"\\\"\\n\",\n        \"            \\n\",\n        \"            for iteration in range(self.max_iter):\\n\",\n        \"                # Termination checks\\n\",\n        \"                if self.deadline and time.time() >= self.deadline:\\n\",\n        \"                    print(\\\"â° Deadline reached\\\")\\n\",\n        \"                    break\\n\",\n        \"                if final_answer_found:\\n\",\n        \"                    print(\\\"âœ… Final answer found at iteration:\\\", iteration)\\n\",\n        \"                    break\\n\",\n        \"                if stop_event and stop_event.is_set():\\n\",\n        \"                    break\\n\",\n        \"                \\n\",\n        \"                # Render conversation to token IDs\\n\",\n        \"                prompt_ids = encoding.render_conversation_for_completion(\\n\",\n        \"                    Conversation.from_messages(messages),\\n\",\n        \"                    Role.ASSISTANT\\n\",\n        \"                )\\n\",\n        \"                max_tokens = self.max_model_len - len(prompt_ids)\\n\",\n        \"                if max_tokens < 1:\\n\",\n        \"                    print(\\\"âš ï¸ Context full!\\\")\\n\",\n        \"                    break\\n\",\n        \"                \\n\",\n        \"                token_buffer = []\\n\",\n        \"                token_buffer_str = \\\"\\\"\\n\",\n        \"                breaking = False\\n\",\n        \"                \\n\",\n        \"                # Stream tokens from the model\\n\",\n        \"                stream = self.client.completions.create(\\n\",\n        \"                    model=self.model,\\n\",\n        \"                    prompt=prompt_ids,\\n\",\n        \"                    max_tokens=max_tokens,\\n\",\n        \"                    temperature=self.temperature,\\n\",\n        \"                    top_p=self.top_p,\\n\",\n        \"                    seed=self.seed + seed_offset,\\n\",\n        \"                    stream=True,\\n\",\n        \"                    extra_body=dict(\\n\",\n        \"                        min_p=self.min_p,\\n\",\n        \"                        stop_token_ids=self.stop_token_ids,\\n\",\n        \"                        return_token_ids=True,\\n\",\n        \"                    ),\\n\",\n        \"                    timeout=360,\\n\",\n        \"                )\\n\",\n        \"                \\n\",\n        \"                try:\\n\",\n        \"                    for chunk in stream:\\n\",\n        \"                        if stop_event and stop_event.is_set():\\n\",\n        \"                            breaking = True\\n\",\n        \"                            break\\n\",\n        \"                        \\n\",\n        \"                        token_chunk = chunk.choices[0].token_ids\\n\",\n        \"                        text_chunk = chunk.choices[0].text\\n\",\n        \"                        \\n\",\n        \"                        if token_chunk:\\n\",\n        \"                            token_buffer.extend(token_chunk)\\n\",\n        \"                            token_buffer_str += text_chunk\\n\",\n        \"                        \\n\",\n        \"                        if self.deadline and time.time() >= self.deadline:\\n\",\n        \"                            breaking = True\\n\",\n        \"                            break\\n\",\n        \"                        \\n\",\n        \"                        if len(token_buffer) > 60_000:\\n\",\n        \"                            print(\\\"âš ï¸ Token limit\\\")\\n\",\n        \"                            breaking = True\\n\",\n        \"                            break\\n\",\n        \"                        \\n\",\n        \"                        # Early stop if boxed detected\\n\",\n        \"                        if \\\"}\\\" in text_chunk and self.extract_boxed_text(token_buffer_str) is not None:\\n\",\n        \"                            final_answer_found = token_buffer_str\\n\",\n        \"                            breaking = True\\n\",\n        \"                            break\\n\",\n        \"                finally:\\n\",\n        \"                    try:\\n\",\n        \"                        stream.close()\\n\",\n        \"                    except Exception:\\n\",\n        \"                        pass\\n\",\n        \"                \\n\",\n        \"                if breaking:\\n\",\n        \"                    break\\n\",\n        \"                \\n\",\n        \"                # Parse any full assistant messages generated\\n\",\n        \"                if token_buffer:\\n\",\n        \"                    new_messages = encoding.parse_messages_from_completion_tokens(\\n\",\n        \"                        token_buffer, Role.ASSISTANT\\n\",\n        \"                    )\\n\",\n        \"                    messages.extend(new_messages)\\n\",\n        \"                \\n\",\n        \"                last_message = messages[-1]\\n\",\n        \"                if last_message.channel == \\\"final\\\" or (token_buffer and token_buffer[-1] == 200002):\\n\",\n        \"                    break\\n\",\n        \"                \\n\",\n        \"                # Python tool execution (deadline-aware)\\n\",\n        \"                if last_message.recipient == \\\"python\\\":\\n\",\n        \"                    tl = _time_left()\\n\",\n        \"                    if tl is not None and tl <= 0:\\n\",\n        \"                        print(\\\"â° Deadline reached before python call\\\")\\n\",\n        \"                        break\\n\",\n        \"                    \\n\",\n        \"                    if tl is None:\\n\",\n        \"                        tool_timeout = None\\n\",\n        \"                    else:\\n\",\n        \"                        tool_timeout = max(1.0, min(20.0, tl - 1.0))\\n\",\n        \"                    \\n\",\n        \"                    print(f\\\"ðŸ Executing Python code... (timeout={tool_timeout})\\\")\\n\",\n        \"                    try:\\n\",\n        \"                        python_tool._ensure_session()\\n\",\n        \"                        out = python_tool._jupyter_session.execute(last_message.content[0].text, timeout=tool_timeout)\\n\",\n        \"                        response_msgs = [python_tool._make_response(out, channel=last_message.channel)]\\n\",\n        \"                        messages.extend(response_msgs)\\n\",\n        \"                    except TimeoutError as e:\\n\",\n        \"                        print(f\\\"âš ï¸ Python timed out: {e} -> reset/recreate kernel now\\\")\\n\",\n        \"                        try:\\n\",\n        \"                            python_tool.reset()\\n\",\n        \"                        except Exception:\\n\",\n        \"                            _recreate_tool(close_old=True)\\n\",\n        \"                        break\\n\",\n        \"                    except Exception as e:\\n\",\n        \"                        print(f\\\"âš ï¸ Python tool execution failed: {e} -> recreate kernel now\\\")\\n\",\n        \"                        _recreate_tool(close_old=True)\\n\",\n        \"                        break\\n\",\n        \"                \\n\",\n        \"                # If a final boxed answer was found during streaming, return that text\\n\",\n        \"                if final_answer_found:\\n\",\n        \"                    return final_answer_found\\n\",\n        \"            \\n\",\n        \"            # Otherwise, return the entire conversation as text\\n\",\n        \"            return encoding.decode_utf8(\\n\",\n        \"                encoding.render_conversation_for_training(\\n\",\n        \"                    Conversation.from_messages(messages),\\n\",\n        \"                    RenderConversationConfig(auto_drop_analysis=False)\\n\",\n        \"                )\\n\",\n        \"            )\\n\",\n        \"        except Exception as e:\\n\",\n        \"            print(f\\\"Error in generation: {e}\\\")\\n\",\n        \"            return \\\"\\\"\\n\",\n        \"        finally:\\n\",\n        \"            # Return to pool OR close\\n\",\n        \"            if python_tool is not None:\\n\",\n        \"                if borrowed_from_pool:\\n\",\n        \"                    try:\\n\",\n        \"                        pool = _get_pool()\\n\",\n        \"                        if isinstance(pool, queue.Queue):\\n\",\n        \"                            pool.put(python_tool)\\n\",\n        \"                        else:\\n\",\n        \"                            python_tool.close()\\n\",\n        \"                    except Exception:\\n\",\n        \"                        try:\\n\",\n        \"                            python_tool.close()\\n\",\n        \"                        except Exception:\\n\",\n        \"                            pass\\n\",\n        \"                else:\\n\",\n        \"                    try:\\n\",\n        \"                        python_tool.close()\\n\",\n        \"                    except Exception:\\n\",\n        \"                        pass\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"    def _inference_parallel(self, prompts: list[str]) -> list[str]:\\n\",\n        \"        \\\"\\\"\\\"Run multiple single_generate_tir in parallel and return all raw responses.\\\"\\\"\\\"\\n\",\n        \"        from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED\\n\",\n        \"        \\n\",\n        \"        stop_event = threading.Event()\\n\",\n        \"        answers_collected: List[int] = []\\n\",\n        \"        raw_responses = [\\\"\\\"] * len(prompts)\\n\",\n        \"        majority_threshold = len(prompts) / 2\\n\",\n        \"        \\n\",\n        \"        def time_left() -> float:\\n\",\n        \"            dl = getattr(self, \\\"deadline\\\", None)\\n\",\n        \"            return float(\\\"inf\\\") if dl is None else (dl - time.time())\\n\",\n        \"        \\n\",\n        \"        print(f\\\"ðŸš€ Sampling {len(prompts)} times (threshold: > {majority_threshold})...\\\")\\n\",\n        \"        executor = ThreadPoolExecutor(max_workers=min(self.k, len(prompts)))\\n\",\n        \"        futures = []\\n\",\n        \"        fut2idx = {}\\n\",\n        \"        \\n\",\n        \"        try:\\n\",\n        \"            for i, p in enumerate(prompts):\\n\",\n        \"                fut = executor.submit(self.single_generate_tir, p, stop_event, i)\\n\",\n        \"                futures.append(fut)\\n\",\n        \"                fut2idx[fut] = i\\n\",\n        \"            \\n\",\n        \"            pending = set(futures)\\n\",\n        \"            majority_reached = False\\n\",\n        \"            \\n\",\n        \"            while pending:\\n\",\n        \"                if time_left() <= 0:\\n\",\n        \"                    stop_event.set()\\n\",\n        \"                    break\\n\",\n        \"                \\n\",\n        \"                timeout = min(0.5, max(0.0, time_left()))\\n\",\n        \"                done, pending = wait(pending, timeout=timeout, return_when=FIRST_COMPLETED)\\n\",\n        \"                \\n\",\n        \"                if not done:\\n\",\n        \"                    continue\\n\",\n        \"                \\n\",\n        \"                for fut in done:\\n\",\n        \"                    idx = fut2idx[fut]\\n\",\n        \"                    try:\\n\",\n        \"                        result_text = fut.result()\\n\",\n        \"                    except Exception as e:\\n\",\n        \"                        print(f\\\"Task exception: {e}\\\")\\n\",\n        \"                        result_text = \\\"\\\"\\n\",\n        \"                    \\n\",\n        \"                    raw_responses[idx] = result_text\\n\",\n        \"                    ans = self.extract_boxed_text(result_text)\\n\",\n        \"                    if ans is not None:\\n\",\n        \"                        answers_collected.append(ans)\\n\",\n        \"                \\n\",\n        \"                counts = Counter(answers_collected)\\n\",\n        \"                if len(counts) > 0:\\n\",\n        \"                    most_common_ans, count = counts.most_common(1)[0]\\n\",\n        \"                    if count > majority_threshold:\\n\",\n        \"                        print(f\\\"ðŸŽ¯ Majority reached! {most_common_ans} appeared {count} times\\\")\\n\",\n        \"                        stop_event.set()\\n\",\n        \"                        majority_reached = True\\n\",\n        \"                        break\\n\",\n        \"                \\n\",\n        \"                if majority_reached:\\n\",\n        \"                    break\\n\",\n        \"            \\n\",\n        \"            if majority_reached and time_left() > 0:\\n\",\n        \"                grace = min(2.0, max(0.0, time_left()))\\n\",\n        \"                wait(pending, timeout=grace)\\n\",\n        \"        finally:\\n\",\n        \"            executor.shutdown(wait=False, cancel_futures=True)\\n\",\n        \"        \\n\",\n        \"        return raw_responses\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"# Initialize the inferencer\\n\",\n        \"inferencer = HarmonyTIRInferencer(\\n\",\n        \"    \\\"/kaggle/input/gpt-oss-120b/transformers/default/1\\\",\\n\",\n        \"    use_budget=USE_BUDGET,\\n\",\n        \"    k=K,\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Wait for the vLLM server to be ready\\n\",\n        \"inferencer.wait_server()\\n\",\n        \"\\n\",\n        \"init_time = time.time()\\n\",\n        \"final_cutoff_time = init_time + 3600  # 1 hour from start\\n\",\n        \"cutoff_times = [int(x) for x in np.linspace(final_cutoff_time, init_time, 50 + 1)]\\n\",\n        \"cutoff_times.pop()  # remove the last element to get exactly 50 cutoff deadlines\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\\n\",\n        \"    \\\"\\\"\\\"Make a prediction.\\\"\\\"\\\"\\n\",\n        \"    global correct_count, total_count, predictions, cutoff_times\\n\",\n        \"    \\n\",\n        \"    question_id = id_.item(0)\\n\",\n        \"    question_text = question.item(0)\\n\",\n        \"    \\n\",\n        \"    print(\\\"------\\\")\\n\",\n        \"    print(f\\\"ID: {question_id}\\\")\\n\",\n        \"    print(f\\\"Question: {question_text[:200]}...\\\")\\n\",\n        \"    \\n\",\n        \"    current_deadline = cutoff_times[-1]\\n\",\n        \"    answer, saved_time = inferencer.inference(question_text, deadline=current_deadline)\\n\",\n        \"    cutoff_times.pop()\\n\",\n        \"    \\n\",\n        \"    # Dynamically recompute cutoff_times and distribute saved_time\\n\",\n        \"    if len(cutoff_times) > 0:\\n\",\n        \"        now = time.time()\\n\",\n        \"        num_remaining = len(cutoff_times)\\n\",\n        \"        base_times = np.linspace(final_cutoff_time, now, num_remaining + 1)\\n\",\n        \"        base_times = base_times[:-1]  # keep only N timestamps\\n\",\n        \"        extra = saved_time / num_remaining\\n\",\n        \"        cutoff_times = [int(t + extra) for t in base_times]\\n\",\n        \"    \\n\",\n        \"    # Store prediction\\n\",\n        \"    predictions[question_id] = answer\\n\",\n        \"    \\n\",\n        \"    # Check accuracy if ground truth available\\n\",\n        \"    total_count += 1\\n\",\n        \"    if question_id in ground_truth:\\n\",\n        \"        gt = ground_truth[question_id]\\n\",\n        \"        is_correct = (answer == gt)\\n\",\n        \"        if is_correct:\\n\",\n        \"            correct_count += 1\\n\",\n        \"        status = \\\"âœ…\\\" if is_correct else \\\"âŒ\\\"\\n\",\n        \"        print(f\\\"Answer: {answer} | Ground Truth: {gt} | {status}\\\")\\n\",\n        \"        print(f\\\"ðŸ“Š Running Accuracy: {correct_count}/{total_count} ({100*correct_count/total_count:.1f}%)\\\")\\n\",\n        \"    else:\\n\",\n        \"        print(f\\\"Answer: {answer}\\\")\\n\",\n        \"    \\n\",\n        \"    print(\\\"------\\\\n\\\")\\n\",\n        \"    return pl.DataFrame({\\\"id\\\": question_id, \\\"answer\\\": answer})\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"# Load reference data and keep ground truth for accuracy calculation\\n\",\n        \"df = pd.read_csv(\\n\",\n        \"    \\\"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\\\"\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Store ground truth answers for accuracy calculation (only in local mode)\\n\",\n        \"ground_truth = dict(zip(df[\\\"id\\\"], df[\\\"answer\\\"])) if \\\"answer\\\" in df.columns else {}\\n\",\n        \"\\n\",\n        \"# Create input file without answers\\n\",\n        \"df.drop(\\\"answer\\\", axis=1, errors=\\\"ignore\\\").to_csv(\\\"reference.csv\\\", index=False)\\n\",\n        \"\\n\",\n        \"# Track predictions for accuracy calculation\\n\",\n        \"predictions = {}\\n\",\n        \"correct_count = 0\\n\",\n        \"total_count = 0\\n\",\n        \"\\n\",\n        \"import kaggle_evaluation.aimo_3_inference_server\\n\",\n        \"inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\\n\",\n        \"\\n\",\n        \"if os.getenv(\\\"KAGGLE_IS_COMPETITION_RERUN\\\"):\\n\",\n        \"    inference_server.serve()\\n\",\n        \"else:\\n\",\n        \"    inference_server.run_local_gateway((\\\"reference.csv\\\",))\\n\",\n        \"\\n\",\n        \"# Print final accuracy summary\\n\",\n        \"if ground_truth and total_count > 0:\\n\",\n        \"    print(\\\"\\\\n\\\" + \\\"=\\\" * 50)\\n\",\n        \"    print(\\\"ðŸ“Š FINAL ACCURACY SUMMARY\\\")\\n\",\n        \"    print(\\\"=\\\" * 50)\\n\",\n        \"    print(f\\\"Correct: {correct_count}/{total_count}\\\")\\n\",\n        \"    print(f\\\"Accuracy: {100*correct_count/total_count:.1f}%\\\")\\n\",\n        \"    print(\\\"=\\\" * 50)\\n\",\n        \"    \\n\",\n        \"    # Show details\\n\",\n        \"    print(\\\"\\\\nDetails:\\\")\\n\",\n        \"    for qid, pred in predictions.items():\\n\",\n        \"        if qid in ground_truth:\\n\",\n        \"            gt = ground_truth[qid]\\n\",\n        \"            status = \\\"âœ…\\\" if pred == gt else \\\"âŒ\\\"\\n\",\n        \"            print(f\\\"  {qid}: pred={pred}, gt={gt} {status}\\\")\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {},\n      \"source\": [\n        \"if train is not None and 'resp' in train.columns and len(continuous_features) > 0:\\n\",\n        \"    print(\\\"Target looks like regression\\\")\\n\",\n        \"    \\n\",\n        \"    # Prepare data for regression\\n\",\n        \"    X_reg = train[continuous_features].fillna(0)\\n\",\n        \"    y_reg = train['resp']\\n\",\n        \"    \\n\",\n        \"    # Use weights if available\\n\",\n        \"    sample_weight_reg = None\\n\",\n        \"    if 'weight' in train.columns:\\n\",\n        \"        sample_weight_reg = train['weight'].values\\n\",\n        \"    \\n\",\n        \"    # Split data\\n\",\n        \"    X_train_reg, X_test_reg, y_train_reg, y_test_reg, sw_train_reg, sw_test_reg = train_test_split(\\n\",\n        \"        X_reg, y_reg, sample_weight_reg, test_size=0.2, random_state=42\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    # Scale features\\n\",\n        \"    scaler_reg = StandardScaler()\\n\",\n        \"    X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\\n\",\n        \"    X_test_reg_scaled = scaler_reg.transform(X_test_reg)\\n\",\n        \"    \\n\",\n        \"    # Train linear regression\\n\",\n        \"    lr = LinearRegression()\\n\",\n        \"    lr.fit(X_train_reg_scaled, y_train_reg, sample_weight=sw_train_reg)\\n\",\n        \"    \\n\",\n        \"    # Predictions\\n\",\n        \"    y_pred_reg = lr.predict(X_test_reg_scaled)\\n\",\n        \"    \\n\",\n        \"    # Evaluate\\n\",\n        \"    mse = mean_squared_error(y_test_reg, y_pred_reg, sample_weight=sw_test_reg)\\n\",\n        \"    r2 = r2_score(y_test_reg, y_pred_reg, sample_weight=sw_test_reg)\\n\",\n        \"    \\n\",\n        \"    print(f\\\"Mean Squared Error: {mse:.6f}\\\")\\n\",\n        \"    print(f\\\"RÂ² Score: {r2:.4f}\\\")\\n\",\n        \"    print(f\\\"RMSE: {np.sqrt(mse):.6f}\\\")\\n\",\n        \"    \\n\",\n        \"    # Regression visualization\\n\",\n        \"    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\\n\",\n        \"    fig.suptitle('Regression Analysis Results', fontsize=16, fontweight='bold')\\n\",\n        \"    \\n\",\n        \"    # Predicted vs Actual\\n\",\n        \"    axes[0].scatter(y_test_reg, y_pred_reg, alpha=0.5, s=1)\\n\",\n        \"    axes[0].plot([y_test_reg.min(), y_test_reg.max()], \\n\",\n        \"                 [y_test_reg.min(), y_test_reg.max()], \\n\",\n        \"                 'r--', lw=2, label='Perfect Prediction')\\n\",\n        \"    axes[0].set_xlabel('Actual Response')\\n\",\n        \"    axes[0].set_ylabel('Predicted Response')\\n\",\n        \"    axes[0].set_title(f'Predicted vs Actual (RÂ² = {r2:.4f})')\\n\",\n        \"    axes[0].legend()\\n\",\n        \"    axes[0].grid(True, alpha=0.3)\\n\",\n        \"    \\n\",\n        \"    # Residuals\\n\",\n        \"    residuals = y_test_reg - y_pred_reg\\n\",\n        \"    axes[1].scatter(y_pred_reg, residuals, alpha=0.5, s=1)\\n\",\n        \"    axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\\n\",\n        \"    axes[1].set_xlabel('Predicted Response')\\n\",\n        \"    axes[1].set_ylabel('Residuals')\\n\",\n        \"    axes[1].set_title('Residual Plot')\\n\",\n        \"    axes[1].grid(True, alpha=0.3)\\n\",\n        \"    \\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    }\n  ],\n  \"metadata\": {\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"language\": \"python\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"name\": \"python\",\n      \"version\": \"3.8.0\"\n    }\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 4\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T12:50:15.627595Z","iopub.execute_input":"2026-01-23T12:50:15.628027Z","iopub.status.idle":"2026-01-23T12:50:15.801275Z","shell.execute_reply.started":"2026-01-23T12:50:15.627997Z","shell.execute_reply":"2026-01-23T12:50:15.800159Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2648060199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;34m\"install_package(\\\"seaborn\\\")\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       ],\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0;34m\"execution_count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnull\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     },\n","\u001b[0;31mNameError\u001b[0m: name 'null' is not defined"],"ename":"NameError","evalue":"name 'null' is not defined","output_type":"error"}],"execution_count":2}]}